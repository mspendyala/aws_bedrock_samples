{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMLTTublsd1cnBezA67G44",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mspendyala/aws_bedrock_samples/blob/main/aws_bedrock_example_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_RZx1aQSur7",
        "outputId": "6c06dd1c-3649-402b-a928-fb1f063ea7a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.36.23-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting botocore<1.37.0,>=1.36.23 (from boto3)\n",
            "  Downloading botocore-1.36.23-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3)\n",
            "  Downloading s3transfer-0.11.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.37.0,>=1.36.23->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.37.0,>=1.36.23->boto3) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.37.0,>=1.36.23->boto3) (1.17.0)\n",
            "Downloading boto3-1.36.23-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.36.23-py3-none-any.whl (13.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.11.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.36.23 botocore-1.36.23 jmespath-1.0.1 s3transfer-0.11.2\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Replace with your AWS credentials (use temporary credentials if possible)\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"\"\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"\"\n",
        "os.environ[\"AWS_REGION\"] = \"us-east-1\"\n",
        "\n",
        "print(\"AWS credentials configured securely in Colab!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWlolsnKS5Y6",
        "outputId": "bfff6949-94cf-48cd-d03f-e0715201b254"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AWS credentials configured securely in Colab!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import json\n",
        "import os\n",
        "from botocore.exceptions import NoCredentialsError\n",
        "\n",
        "\n",
        "# Load AWS credentials from Colab environment variables\n",
        "aws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
        "aws_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
        "aws_region = os.getenv(\"AWS_REGION\", \"us-east-1\")\n",
        "\n",
        "\n",
        "# Initialize S3 & Bedrock Clients\n",
        "s3_client = boto3.client(\n",
        "    \"s3\",\n",
        "    aws_access_key_id=aws_access_key,\n",
        "    aws_secret_access_key=aws_secret_key,\n",
        "    region_name=aws_region,\n",
        ")\n",
        "\n",
        "bedrock_client = boto3.client(\n",
        "    \"bedrock-runtime\",\n",
        "    aws_access_key_id=aws_access_key,\n",
        "    aws_secret_access_key=aws_secret_key,\n",
        "    region_name=aws_region,\n",
        ")\n",
        "\n",
        "# S3 Bucket and File\n",
        "S3_BUCKET = \"my-bedrock-knowledge\"\n",
        "S3_FILE_KEY = \"knowledge_base.json\"\n",
        "\n",
        "def check_s3_file():\n",
        "    \"\"\"Check if the knowledge base file exists in S3\"\"\"\n",
        "    try:\n",
        "        response = s3_client.list_objects_v2(Bucket=S3_BUCKET)\n",
        "        if \"Contents\" in response:\n",
        "            print(\"\\n✅ Files in S3 bucket:\")\n",
        "            for obj in response[\"Contents\"]:\n",
        "                print(f\"- {obj['Key']}\")\n",
        "                if obj[\"Key\"] == S3_FILE_KEY:\n",
        "                    return True\n",
        "        print(f\"⚠️ File {S3_FILE_KEY} NOT found in S3 bucket `{S3_BUCKET}`. Upload it first.\")\n",
        "        return False\n",
        "    except ClientError as e:\n",
        "        print(f\"❌ Error checking S3: {e}\")\n",
        "        return False\n",
        "\n",
        "def fetch_knowledge_from_s3():\n",
        "    \"\"\"Retrieve knowledge data from S3\"\"\"\n",
        "    try:\n",
        "        response = s3_client.get_object(Bucket=S3_BUCKET, Key=S3_FILE_KEY)\n",
        "        knowledge_data = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n",
        "        return knowledge_data\n",
        "    except NoCredentialsError:\n",
        "        print(\"❌ AWS credentials not found!\")\n",
        "        return []\n",
        "    except ClientError as e:\n",
        "        print(f\"❌ Error fetching knowledge base: {e}\")\n",
        "        return []\n",
        "\n",
        "def retrieve_relevant_context(user_query, knowledge_base):\n",
        "    \"\"\"Simple keyword-based retrieval from S3 knowledge base\"\"\"\n",
        "    relevant_context = []\n",
        "    for entry in knowledge_base:\n",
        "        if user_query.lower() in entry[\"question\"].lower():\n",
        "            relevant_context.append(entry[\"answer\"])\n",
        "\n",
        "    return \"\\n\".join(relevant_context) if relevant_context else \"No relevant knowledge found.\"\n",
        "\n",
        "def query_bedrock(prompt):\n",
        "    \"\"\"Query Amazon Bedrock with the correct prompt format for Claude\"\"\"\n",
        "    formatted_prompt = f'\\n\\nHuman: {prompt}\\n\\nAssistant:'\n",
        "    print(f\"\\nPrompt sent to Bedrock:\\n{formatted_prompt}\")\n",
        "    try:\n",
        "        response = bedrock_client.invoke_model(\n",
        "            modelId=\"anthropic.claude-instant-v1\",  # Change model if needed\n",
        "            body=json.dumps({\n",
        "                \"prompt\": formatted_prompt,\n",
        "                \"max_tokens_to_sample\": 500\n",
        "            })\n",
        "        )\n",
        "        result = json.loads(response[\"body\"].read().decode(\"utf-8\"))\n",
        "        return result.get(\"completion\", \"No response from model\")\n",
        "    except ClientError as e:\n",
        "        print(f\"Error calling Bedrock: {e}\")\n",
        "        return \"Error in Bedrock call.\"\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main workflow for RAG using S3 & Amazon Bedrock\"\"\"\n",
        "    user_query = \"What is Retrieval-Augmented Generation?\"\n",
        "\n",
        "    # Step 1: Check if the knowledge base file exists in S3\n",
        "    if not check_s3_file():\n",
        "        return\n",
        "\n",
        "    # Step 2: Fetch knowledge data from S3\n",
        "    knowledge_base = fetch_knowledge_from_s3()\n",
        "\n",
        "    # Step 3: Retrieve relevant knowledge\n",
        "    retrieved_context = retrieve_relevant_context(user_query, knowledge_base)\n",
        "\n",
        "    # Step 4: Create final prompt for Bedrock\n",
        "    final_prompt = f\"Using the following information:\\n{retrieved_context}\\n\\nAnswer this: {user_query}\"\n",
        "\n",
        "    # Step 5: Query Bedrock with RAG-enhanced prompt\n",
        "    response_rag = query_bedrock(final_prompt)\n",
        "\n",
        "    # Step 6: Query Bedrock without retrieval (Vanilla LLM)\n",
        "    response_vanilla = query_bedrock(user_query)\n",
        "\n",
        "    print(\"\\n🔹 **Vanilla Bedrock Response:**\")\n",
        "    print(response_vanilla)\n",
        "    print(\"\\n🔹 **RAG-Enhanced Response (Using S3):**\")\n",
        "    print(response_rag)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zObrCgGUKAU",
        "outputId": "fdacc4ce-285c-43b9-d266-5074f9b9a5d0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Files in S3 bucket:\n",
            "- knowledge_base.json\n",
            "\n",
            "Prompt sent to Bedrock:\n",
            "\n",
            "\n",
            "Human: Using the following information:\n",
            "RAG is an AI technique that retrieves relevant knowledge before generating responses to improve accuracy.\n",
            "\n",
            "Answer this: What is Retrieval-Augmented Generation?\n",
            "\n",
            "Assistant:\n",
            "\n",
            "Prompt sent to Bedrock:\n",
            "\n",
            "\n",
            "Human: What is Retrieval-Augmented Generation?\n",
            "\n",
            "Assistant:\n",
            "\n",
            "🔹 **Vanilla Bedrock Response:**\n",
            " Retrieval-augmented generation (or retrieve-and-refine) is a technique used in language models where candidate text is retrieved from a large corpus and then refined or modified to better suit the context of the conversation. \n",
            "\n",
            "Some key aspects of retrieval-augmented generation include:\n",
            "\n",
            "- Using a dense semantic search or retrieval model to find relevant text from a large corpus that is similar in meaning to the context of the conversation. This text could be full passages, sentences or snippets.\n",
            "\n",
            "- Taking the retrieved text as a starting point or \"draft\" response and modifying it through natural language generation techniques like text infilling, paraphrasing, deleting or reordering text to better fit the specific context and goals of the conversation. \n",
            "\n",
            "- Leveraging both retrieval and generation capabilities together - retrieval to provide a relevant starting point and generation to refine it. This helps improve fluency, coherence and specificity compared to models that rely only on generation.\n",
            "\n",
            "- The retrieved corpus can come from a wide range of sources like websites, books or even the model's own training conversations. This additional context often helps the model form better responses.\n",
            "\n",
            "So in summary, retrieval-augmented generation uses retrieval to find a suitable draft response which is then refined through generation for a more customized and human-like final response.\n",
            "\n",
            "🔹 **RAG-Enhanced Response (Using S3):**\n",
            " Based on the information provided, Retrieval-Augmented Generation (RAG) is an AI technique that retrieves relevant knowledge before generating responses to improve accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vMSts5CjURlq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}